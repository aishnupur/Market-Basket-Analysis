mat1 <- matrix(data = c(0,1/2,0,1/2,0,0,0,0,1,0,0,0,1/2,0,0,0,1/2,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0), nrow = 6, ncol = 6)
View(mat1)
tel1 = 0.1
tel2 = 0.2
tel3 = 0.4
mat2 <- matrix(data = c(1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6,1/6), nrow = 6, ncol = 6)
current_rank <- matrix(data = c(1,1,1,1,1,1),nrow = 6,ncol = 1)
flag <- FALSE
k = 0
while (flag==FALSE) {
m=0
new_rank <- mat1 %*% current_rank
difference <- new_rank - current_rank
current_rank <- new_rank
for (i in 1:6) {
if(difference[i,1] > 0.001 || difference[i,1]< -0.001)
{
m = m+1
}
if(m>0)
{
flag = FALSE
}
else
{
flag = TRUE
}
}
k= k+1
}
print(k)
mat3 = 0.1 * mat1 + 0.9 * mat2
current_rank2 <- matrix(data = c(1,1,1,1,1,1),nrow = 6,ncol = 1)
flag2 <- FALSE
k2 = 0
while (flag2==FALSE) {
m2=0
new_rank2 <- mat3 %*% current_rank2
difference2 <- new_rank2 - current_rank2
current_rank2 <- new_rank2
for (i in 1:6) {
if(difference2[i,1] > 0.001 || difference2[i,1]< -0.001)
{
m2 = m2+1
}
if(m2>0)
{
flag2 = FALSE
}
else
{
flag2 = TRUE
}
}
k2= k2+1
}
print(k2)
mat4 = 0.2 * mat1 + 0.8 * mat2
current_rank3 <- matrix(data = c(1,1,1,1,1,1),nrow = 6,ncol = 1)
flag3 <- FALSE
k3 = 0
while (flag3==FALSE) {
m3=0
new_rank3 <- mat4 %*% current_rank3
difference3 <- new_rank3 - current_rank3
current_rank3 <- new_rank3
for (i in 1:6) {
if(difference3[i,1] > 0.001 || difference3[i,1]< -0.001)
{
m3 = m3+1
}
if(m3>0)
{
flag3 = FALSE
}
else
{
flag3 = TRUE
}
}
k3= k3+1
}
print(k3)
mat5 = 0.4 * mat1 + 0.6 * mat2
current_rank4 <- matrix(data = c(1,1,1,1,1,1),nrow = 6,ncol = 1)
flag4 <- FALSE
k4 = 0
while (flag4==FALSE) {
m4=0
new_rank4 <- mat5 %*% current_rank4
difference4 <- new_rank4 - current_rank4
current_rank4 <- new_rank4
for (i in 1:6) {
if(difference4[i,1] > 0.001 || difference4[i,1]< -0.001)
{
m4 = m4+1
}
if(m4>0)
{
flag4 = FALSE
}
else
{
flag4 = TRUE
}
}
k4= k4+1
}
print(k4)
setwd("C:/Users/golak/OneDrive/Desktop/College ML assignments/Market-Basket-Analysis")
library(arules)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
dataset2 <- data.frame(train_data$VisitNumber, train_data$Upc)
names(dataset2) <- c("transaction", "product")
library(plyr)
ordered_data <- ddply(dataset2,c("transaction"),
function(df1)paste(df1$product,collapse = ","))
write.csv(ordered_data,"ordered_data.csv",quote = FALSE,row.names = TRUE)
txn = read.transactions(file = "ordered_data.csv",rm.duplicates = TRUE,format = "basket",sep = ",",cols=1)
basket_rules <- apriori(txn,parameter = list(sup = 0.0006,conf = 0.5,target = "rules"))
inspect(basket_rules)
#inspect(sort(basket_rules,by='lift')[1:100])
summary(basket_rules)
basket_rules <- basket_rules[!is.redundant(basket_rules)]
rules_dt <- data.table( lhs = labels( lhs(basket_rules) ),
rhs = labels( rhs(basket_rules) ),
quality(basket_rules) )[ order(-lift), ]
library(data.table)
library("data.table", lib.loc="~/R/win-library/3.5")
rules_dt <- data.table( lhs = labels( lhs(basket_rules) ),
rhs = labels( rhs(basket_rules) ),
quality(basket_rules) )[ order(-lift), ]
View(rules_dt)
library(plotly)
plotly_arules(basket_rules)
library(arules)
library(data.table)
library(dplyr)
library(ggplot2)
library(knitr)
install.packages("knitr")
library(knitr)
install.packages("stringr")
library(stringr)
install.packages("DT")
library(DT)
library(plotly)
library(arulesViz)
install.packages("arulesViz")
library(arulesViz)
library(visNetwork)
library(igraph)
library(kableExtra)
install.packages("kableExtra")
library(kableExtra)
library(arules)
library(data.table)
library(dplyr)
library(ggplot2)
library(knitr)
library(stringr)
library(DT)
library(plotly)
library(arulesViz)
library(visNetwork)
library(igraph)
library(kableExtra)
arules::itemFrequencyPlot(txn,
topN=20,
col=brewer.pal(8,'Pastel2'),
main='Relative Item Frequency Plot',
type="relative",
ylab="Item Frequency (Relative)")
install.packages("RColorBrewer")
library(RColorBrewer)
arules::itemFrequencyPlot(txn,
topN=20,
col=brewer.pal(8,'Pastel2'),
main='Relative Item Frequency Plot',
type="relative",
ylab="Item Frequency (Relative)")
basket_rules <- basket_rules[!is.redundant(basket_rules)]
rules_dt <- data.table( lhs = labels( lhs(basket_rules) ),
rhs = labels( rhs(basket_rules) ),
quality(basket_rules) )[ order(-lift), ]
plotly_arules(basket_rules)
sel <- plot(basket_rules, measure=c("support", "lift"),
shading = "confidence",
interactive = TRUE)
subrules2 <- head(sort(basket_rules, by="confidence"),20)
ig <- plot( subrules2, method="graph", control=list(type="items") )
ig_df <- get.data.frame( ig, what = "both" )
nodesv %>%
visNodes(size = 10) %>%
visLegend() %>%
visEdges(smooth = FALSE) %>%
visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
visInteraction(navigationButtons = TRUE) %>%
visEdges(arrows = 'from') %>%
visPhysics(
solver = "barnesHut",
maxVelocity = 35,
forceAtlas2Based = list(gravitationalConstant = -6000)
)
subrules2 <- head(sort(basket_rules, by="confidence"))
ig <- plot( subrules2, method="graph", control=list(type="items") )
subrules2 <- head(sort(basket_rules, by="support"))
ig <- plot( subrules2, method="graph", control=list(type="items") )
dataset3 <- data.frame(train_data$Upc, train_data$DepartmentDescription)
View(dataset3)
subrules2 <- head(sort(basket_rules, by="lift"))
ig <- plot( subrules2, method="graph", control=list(type="items") )
subrules2 <- head(sort(basket_rules, by="confidence"))
ig <- plot( subrules2, method="graph", control=list(type="items") )
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
train_data$TripType <- factor(train_data$TripType)
#############################DATA PREPROCESSING#########################################
sum(is.na(train_data$FinelineNumber))
row.has.na <- apply(train_data, 1, function(x){any(is.na(x))})
sum(row.has.na)
train_data <- train_data[!row.has.na,]
rownames(train_data) <- seq(length=nrow(train_data))
summary(train_data)
library(caTools)
set.seed(123)
split = sample.split(train_data$TripType, SplitRatio = 0.80)
training_set2 = subset(train_data, split == TRUE)
test_set1 = subset(train_data, split == FALSE)
test_set2 <- test_set1[,2:7]
rownames(training_set2) <- seq(length=nrow(training_set2))
rownames(test_set1) <- seq(length=nrow(test_set1))
rownames(test_set2) <- seq(length=nrow(test_set2))
library(e1071)
classifier_nb = naiveBayes(TripType~.,
data = training_set2)
pred_nb = predict(classifier_nb,test_set2)
cm_nb = table(test_set1[,1], pred_nb)
cm_nb
#install.packages('caret')
library(caret)
confusionMatrix(cm_nb)
View(test_set2)
library(e1071)
classifier_nb = naiveBayes(TripType~.,
data = training_set2[-7])
pred_nb = predict(classifier_nb,test_set2[1:5])
cm_nb = table(test_set1[,1], pred_nb)
cm_nb
#install.packages('caret')
library(caret)
confusionMatrix(cm_nb)
training_set2[-7]
View(training_set2)
training_set2 <- training_set2[,-c(5,7)]
View(training_set2)
View(test_set2)
test_set2 <- test_set2[,-c(4,6)]
library(e1071)
classifier_nb = naiveBayes(TripType~.,
data = training_set2)
pred_nb = predict(classifier_nb,test_set2)
cm_nb = table(test_set1[,1], pred_nb)
cm_nb
#install.packages('caret')
library(caret)
confusionMatrix(cm_nb)
library(xgboost)
training_set2[] <- lapply(training_set2, function(x) {
if(is.factor(x)) as.numeric(as.character(x)) else x
})
dtrain <- xgb.DMatrix(data = as.matrix(training_set2), label = training_set2$TripType)
classifier_xg = xgboost(data = dtrain,
max.depth = 10,
eta = 1, nthread = 4, nrounds = 4,verbose = 1,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 38)
pred_xg = predict(classifier_xg,data.matrix(test_set1[,2:7]))
View(test_set1)
pred_xg = predict(classifier_xg,data.matrix(test_set1[,2:6]))
xgbModel <- xgbnoost(data = data.matrix(training_set2[,2:5]), label = data.matrix(training_set2[,1]),
max_depth = 80,
eta = 1, nthread = 8, nrounds = 1,
objective = "multi:softmax",
eval_metric = "mlogloss",
num_class = 38)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:5]), label = data.matrix(training_set2[,1]),
max_depth = 80,
eta = 1, nthread = 8, nrounds = 1,
objective = "multi:softmax",
eval_metric = "mlogloss",
num_class = 38)
predicted.labels_XGB <- predict(xgbModel, data.matrix(test_set2))
cm_xg = table(test_set1[,1], predicted.labels_XGB)
library(caret)
confusionMatrix(cm_xg)
install.packages("mltools")
library(mltools)
library(data.table)
trip_type_1h <- one_hot(as.data.table(train_data$TripType))
View(trip_type_1h)
as.data.frame(trip_type_1h)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
train_data$TripType <- factor(train_data$TripType)
#############################DATA PREPROCESSING#########################################
sum(is.na(train_data$FinelineNumber))
row.has.na <- apply(train_data, 1, function(x){any(is.na(x))})
sum(row.has.na)
train_data <- train_data[!row.has.na,]
rownames(train_data) <- seq(length=nrow(train_data))
summary(train_data)
View(train_data)
train_data <- train_data[,-c(5,7)]
View(test_data)
test_data <- test_data[,-c(4,6)]
library(mltools)
library(data.table)
trip_type_1h <- one_hot(as.data.table(train_data$TripType))
as.data.frame(trip_type_1h)
train_data_1h <- rbind(train_data,trip_type_1h)
dataset2 <- data.frame(train_data[,1:5], trip_type_1h[,1:38])
View(dataset2)
dataset2 <- data.frame(train_data[,2:5], trip_type_1h[,1:38])
View(dataset2)
train_data_1h <- data.frame(train_data[,2:5], trip_type_1h[,1:38])
library(xgboost)
View(train_data_1h)
library(caTools)
set.seed(123)
split = sample.split(train_data_1h, SplitRatio = 0.80)
split = sample.split(train_data_1h[5:42], SplitRatio = 0.80)
library(caTools)
set.seed(123)
split_1h = sample.split(train_data_1h[5:42], SplitRatio = 0.80)
training_set2_1h = subset(train_data_1h, split == TRUE)
test_set1_1h = subset(train_data_1h, split == FALSE)
View(trip_type_1h)
View(test_set1_1h)
View(test_data)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
train_data$TripType <- factor(train_data$TripType)
sum(is.na(train_data$FinelineNumber))
row.has.na <- apply(train_data, 1, function(x){any(is.na(x))})
sum(row.has.na)
train_data <- train_data[!row.has.na,]
rownames(train_data) <- seq(length=nrow(train_data))
summary(train_data)
train_data <- train_data[,-c(5,7)]
test_data <- test_data[,-c(4,6)]
library(mltools)
library(data.table)
trip_type_1h <- one_hot(as.data.table(train_data$TripType))
as.data.frame(trip_type_1h)
train_data_1h <- data.frame(train_data[,2:5], trip_type_1h[,1:38])
library(caTools)
set.seed(123)
split_1h = sample.split(train_data_1h[5:42], SplitRatio = 0.80)
training_set2_1h = subset(train_data_1h, split == TRUE)
dplyr::filter()
dplyr::filter(train_data_1h)
training_set2_1h = subset(train_data_1h, split == TRUE)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
train_data$TripType <- factor(train_data$TripType)
#############################DATA PREPROCESSING#########################################
sum(is.na(train_data$FinelineNumber))
row.has.na <- apply(train_data, 1, function(x){any(is.na(x))})
sum(row.has.na)
train_data <- train_data[!row.has.na,]
rownames(train_data) <- seq(length=nrow(train_data))
summary(train_data)
train_data <- train_data[,-c(5,7)]
test_data <- test_data[,-c(4,6)]
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
train_data$TripType <- factor(train_data$TripType)
sum(is.na(train_data$FinelineNumber))
row.has.na <- apply(train_data, 1, function(x){any(is.na(x))})
sum(row.has.na)
train_data <- train_data[!row.has.na,]
rownames(train_data) <- seq(length=nrow(train_data))
summary(train_data)
train_data <- train_data[,-c(5,7)]
test_data <- test_data[,-c(4,6)]
library(caTools)
set.seed(123)
split = sample.split(train_data$TripType, SplitRatio = 0.80)
training_set2 = subset(train_data, split == TRUE)
test_set1 = subset(train_data, split == FALSE)
test_set2 <- test_set1[,2:7]
rownames(training_set2) <- seq(length=nrow(training_set2))
rownames(test_set1) <- seq(length=nrow(test_set1))
rownames(test_set2) <- seq(length=nrow(test_set2))
test_set2 <- test_set1[,2:5]
rownames(test_set2) <- seq(length=nrow(test_set2))
library(xgboost)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:5]),
label = data.matrix(training_set2[,1]),
max_depth = 12,
eta = 1, nthread = 8, nrounds = 300,
objective = "multi:softmax",
eval_metric = "mlogloss",
num_class = 38,
eta = 0.1,
early_stopping_rounds = 10,
min_child_weight = 3)
predicted.labels_XGB <- predict(xgbModel, data.matrix(test_set2))
cm_xg = table(test_set1[,1], predicted.labels_XGB)
library(caret)
confusionMatrix(cm_xg)
