train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data$TripType <- as.factor(train_data$TripType)
library(caTools)
set.seed(123)
split = sample.split(train_data$TripType, SplitRatio = 0.80)
training_set2 = subset(train_data, split == TRUE)
test_set1 = subset(train_data, split == FALSE)
test_set2 <- test_set1[,2:7]
####################################NAIVE BAYES#########################################
library(e1071)
classifier_nb = naiveBayes(TripType~.,
data = training_set2)
pred_nb = predict(classifier_nb,test_set2)
cm_nb = table(test_set1[,1], pred_nb)
cm_nb
#install.packages('caret')
library(caret)
confusionMatrix(cm_nb)
install.packages("xgboost")
library(xgboost)
classifier_xg = xgboost(data = training_set2,
label = training_set2$TripType,
max.depth = 3,
eta = 1, nthread = 4, nrounds = 2)
training_set2_mat <- data.matrix(training_set2)
classifier_xg = xgboost(data = training_set2,
label = training_set2$TripType,
max.depth = 3,
eta = 1, nthread = 4, nrounds = 2)
View(training_set2_mat)
classifier_xg = xgboost(data = training_set2_mat,
label = training_set2_mat$TripType,
max.depth = 3,
eta = 1, nthread = 4, nrounds = 2)
classifier_xg = xgboost(data = training_set2_mat,
label = TripType,
max.depth = 3,
eta = 1, nthread = 4, nrounds = 2)
library(xgboost)
training_set2_mat <- data.matrix(training_set2)
classifier_xg = xgboost(data = training_set2_mat,
label = training_set2_mat$TripType,
max.depth = 3,
eta = 1, nthread = 4, nrounds = 2)
is.atomic(training_set2_mat$TripType)
is.atomic(training_set2_mat)
dtrain <- xgb.DMatrix(data = training_set2, label = training_set2$TripType)
View(training_set2)
dtrain <- xgb.DMatrix(data = data.frame(training_set2), label = training_set2$TripType)
dim(training_set2)
dim(training_set2$data)
training_set2$data
dtrain <- xgb.DMatrix(data = as.matrix(training_set2), label = training_set2$TripType)
training_set3[] <- lapply(training_set2, function(x) {
if(is.factor(x)) as.numeric(as.character(x)) else x
})
training_set2[] <- lapply(training_set2, function(x) {
if(is.factor(x)) as.numeric(as.character(x)) else x
})
dtrain <- xgb.DMatrix(data = as.matrix(training_set2), label = training_set2$TripType)
classifier_xg = xgboost(data = dtrain,
max.depth = 3,
eta = 1, nthread = 4, nrounds = 2,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 38)
classifier_xg = xgboost(data = dtrain,
max.depth = 3,
eta = 1, nthread = 4, nrounds = 2,
objective = "multi:softprob",
num_class = 38)
classifier_xg = xgboost(data = dtrain,
max.depth = 3,
eta = 1, nthread = 4, nrounds = 2,
num_class = 38)
classifier_xg = xgboost(data = dtrain,
max.depth = 3,
eta = 1, nthread = 4, nrounds = 2,
objective = "multi:softprob")
classifier_xg = xgboost(data = dtrain,
max.depth = 3,
eta = 1, nthread = 4, nrounds = 2)
classifier_xg = xgboost(data = dtrain,
max.depth = 3,
eta = 1, nthread = 4, nrounds = 2,verbose = 1)
classifier_xg = xgboost(data = dtrain,
max.depth = 10,
eta = 1, nthread = 4, nrounds = 2,verbose = 1)
classifier_xg = xgboost(data = dtrain,
max.depth = 50,
eta = 1, nthread = 4, nrounds = 2,verbose = 1)
classifier_xg = xgboost(data = dtrain,
max.depth = 20,
eta = 1, nthread = 4, nrounds = 2,verbose = 1)
classifier_xg = xgboost(data = dtrain,
max.depth = 8,
eta = 1, nthread = 4, nrounds = 2,verbose = 1)
classifier_xg = xgboost(data = dtrain,
max.depth = 9,
eta = 1, nthread = 4, nrounds = 2,verbose = 1)
classifier_xg = xgboost(data = dtrain,
max.depth = 10,
eta = 1, nthread = 4, nrounds = 4,verbose = 1)
classifier_xg = xgboost(data = dtrain,
max.depth = 10,
eta = 1, nthread = 4, nrounds = 10,verbose = 1)
classifier_xg = xgboost(data = dtrain,
max.depth = 12,
eta = 1, nthread = 4, nrounds = 10,verbose = 1)
classifier_xg = xgboost(data = dtrain,
max.depth = 10,
eta = 1, nthread = 4, nrounds = 4,verbose = 1)
pred_xg = predict(classifier_xg,test_set2)
pred_xg = predict(classifier_xg,data.matrix(test_set2))
pred_xg = predict(classifier_xg,data.matrix(test_set1[,2:7]))
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]), max_depth = 10, eta = 1, nthread = 4, nrounds = 4)
predicted.labels_XGB <- predict(xgbModel, data.matrix(test_set2))
predicted.labels_XGB <- predict(xgbModel, data.matrix(test_set2))
cm_xg = table(test_set1[,1], predicted.labels_XGB)
cm_xg
library(caret)
confusionMatrix(cm_xg)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]), max_depth = 10, eta = 1, nthread = 4, nrounds = 4,
"objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = 38)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]), max_depth = 10, eta = 1, nthread = 4, nrounds = 4,
"objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = 37)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]), max_depth = 10, eta = 1, nthread = 4, nrounds = 4,
"objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"num_class" = 39)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]), max_depth = 10, eta = 1, nthread = 4, nrounds = 4,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 39)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]), max_depth = 10, eta = 1, nthread = 4, nrounds = 4,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 38)
unique(training_set2$TripType)
View(train_data)
View(training_set2)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
View(train_data)
library(caTools)
set.seed(123)
split = sample.split(train_data$TripType, SplitRatio = 0.80)
training_set2 = subset(train_data, split == TRUE)
test_set1 = subset(train_data, split == FALSE)
test_set2 <- test_set1[,2:7]
library(xgboost)
training_set2[] <- lapply(training_set2, function(x) {
if(is.factor(x)) as.numeric(as.character(x)) else x
})
dtrain <- xgb.DMatrix(data = as.matrix(training_set2), label = training_set2$TripType)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]), max_depth = 10, eta = 1, nthread = 4, nrounds = 4,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 38)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
library(caTools)
set.seed(123)
split = sample.split(train_data$TripType, SplitRatio = 0.80)
training_set2 = subset(train_data, split == TRUE)
test_set1 = subset(train_data, split == FALSE)
test_set2 <- test_set1[,2:7]
View(train_data)
train_data$TripType + 1
train_data$TripType - 1
train_data$TripType - 1
View(train_data)
train_data$TripType <- train_data$TripType - 1
View(train_data)
View(train_data)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
View(train_data)
library(caTools)
set.seed(123)
split = sample.split(train_data$TripType, SplitRatio = 0.80)
training_set2 = subset(train_data, split == TRUE)
test_set1 = subset(train_data, split == FALSE)
test_set2 <- test_set1[,2:7]
library(xgboost)
training_set2[] <- lapply(training_set2, function(x) {
if(is.factor(x)) as.numeric(as.character(x)) else x
})
dtrain <- xgb.DMatrix(data = as.matrix(training_set2), label = training_set2$TripType)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]), max_depth = 10, eta = 1, nthread = 4, nrounds = 4,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 38)
predicted.labels_XGB <- predict(xgbModel, data.matrix(test_set2))
cm_xg = table(test_set1[,1], predicted.labels_XGB)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
library(caTools)
set.seed(123)
split = sample.split(train_data$TripType, SplitRatio = 0.80)
training_set2 = subset(train_data, split == TRUE)
test_set1 = subset(train_data, split == FALSE)
test_set2 <- test_set1[,2:7]
library(xgboost)
training_set2[] <- lapply(training_set2, function(x) {
if(is.factor(x)) as.numeric(as.character(x)) else x
})
dtrain <- xgb.DMatrix(data = as.matrix(training_set2), label = training_set2$TripType)
classifier_xg = xgboost(data = dtrain,
max.depth = 10,
eta = 1, nthread = 4, nrounds = 4,verbose = 1,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 38)
pred_xg = predict(classifier_xg,data.matrix(test_set1[,2:7]))
predicted.labels_XGB <- predict(classifier_xg, data.matrix(test_set2))
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]), max_depth = 10, eta = 1, nthread = 4, nrounds = 4,
objective = "multi:softprob",
eval_metric = "mlogloss",
num_class = 38)
#max_depth = 2, eta = 1, verbose = 2
predicted.labels_XGB <- predict(xgbModel, data.matrix(test_set2))
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]), max_depth = 10, eta = 1, nthread = 4, nrounds = 4,
objective = "multi:softprob",
eval_metric = "mlogloss")
predicted.labels_XGB <- matrix(predicted.labels_XGB, ncol=38, byrow=TRUE)
View(predicted.labels_XGB)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]), max_depth = 10, eta = 1, nthread = 4, nrounds = 4,
objective = "multi:softmax",
eval_metric = "mlogloss",
num_class = 38)
predicted.labels_XGB <- predict(xgbModel, data.matrix(test_set2))
cm_xg = table(test_set1[,1], predicted.labels_XGB)
library(caret)
confusionMatrix(cm_xg)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]),
max_depth = 10,
eta = 1, nthread = 2, nrounds = 1,
objective = "multi:softmax",
eval_metric = "mlogloss",
num_class = 38)
predicted.labels_XGB <- predict(xgbModel, data.matrix(test_set2))
cm_xg = table(test_set1[,1], predicted.labels_XGB)
library(caret)
confusionMatrix(cm_xg)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]),
max_depth = 2,
eta = 1, nthread = 2, nrounds = 1,
objective = "multi:softmax",
eval_metric = "mlogloss",
num_class = 38)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]),
max_depth = 5,
eta = 1, nthread = 2, nrounds = 1,
objective = "multi:softmax",
eval_metric = "mlogloss",
num_class = 38)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]),
max_depth = 15,
eta = 1, nthread = 2, nrounds = 1,
objective = "multi:softmax",
eval_metric = "mlogloss",
num_class = 38)
#max_depth = 2, eta = 1, verbose = 2
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]),
max_depth = 30,
eta = 1, nthread = 2, nrounds = 1,
objective = "multi:softmax",
eval_metric = "mlogloss",
num_class = 38)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]),
max_depth = 60,
eta = 1, nthread = 2, nrounds = 1,
objective = "multi:softmax",
eval_metric = "mlogloss",
num_class = 38)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]),
max_depth = 60,
eta = 1, nthread = 8, nrounds = 4,
objective = "multi:softmax",
eval_metric = "mlogloss",
num_class = 38)
xgbModel <- xgboost(data = data.matrix(training_set2[,2:7]), label = data.matrix(training_set2[,1]),
max_depth = 80,
eta = 1, nthread = 8, nrounds = 1,
objective = "multi:softmax",
eval_metric = "mlogloss",
num_class = 38)
predicted.labels_XGB <- predict(xgbModel, data.matrix(test_set2))
cm_xg = table(test_set1[,1], predicted.labels_XGB)
library(caret)
confusionMatrix(cm_xg)
install.packages('R6Class')
install.packages('r6class')
install.packages('R6')
install.packages("R6")
library(R6)
xgb_trainer <- XGBTrainer$new()
gst <-GridSearchCV$new(trainer = xgb_trainer,
parameters = list(n_estimators = c(100),
max_depth = c(5,2,10,50,60,80,100),
eta = 1, nthread = 8, nrounds = c(1,2,3)),
n_folds = 3,
scoring = c('accuracy','rmse'))
xgb.grid <- expand.grid(nrounds = 1000,
eta = c(0.01,0.05,0.1),
max_depth = c(2,4,6,8,10,14)
)
View(xgb.grid)
xgb.grid <- expand.grid(max_depth = c(5,2,10,50,60,80,100),
eta = 1, nthread = 8, nrounds = c(1,2,3)
)
xgb.grid <- expand.grid(max_depth = c(5,2,10,50,60,80,100),
eta = 1, nthread = 8, nrounds = c(1,2,3),
scoring = c('accuracy','rmse')
)
sum(is.na(train_data$Upc))
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
sum(is.na(train_data$Upc))
sum(is.na(train_data$ScanCount))
sum(is.na(train_data$VisitNumber))
sum(is.na(train_data$TripType))
sum(is.na(train_data$DepartmentDescription))
sum(is.na(train_data$Upc))
row.has.na <- apply(train_data, 1, function(x){any(is.na(x))})
sum(row.has.na)
train_data <- train_data[!row.has.na,]
library(caTools)
set.seed(123)
split = sample.split(train_data$TripType, SplitRatio = 0.80)
training_set2 = subset(train_data, split == TRUE)
test_set1 = subset(train_data, split == FALSE)
test_set2 <- test_set1[,2:7]
library(e1071)
classifier_nb = naiveBayes(TripType~.,
data = training_set2)
pred_nb = predict(classifier_nb,test_set2)
cm_nb = table(test_set1[,1], pred_nb)
cm_nb
#install.packages('caret')
library(caret)
confusionMatrix(cm_nb)
pred_nb = predict(classifier_nb,test_set2)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
sum(is.na(train_data$Upc))
row.has.na <- apply(train_data, 1, function(x){any(is.na(x))})
sum(row.has.na)
train_data <- train_data[!row.has.na,]
View(train_data)
rownames(train_data) <- seq(length=nrow(train_data))
View(train_data)
library(caTools)
set.seed(123)
split = sample.split(train_data$TripType, SplitRatio = 0.80)
training_set2 = subset(train_data, split == TRUE)
test_set1 = subset(train_data, split == FALSE)
test_set2 <- test_set1[,2:7]
library(e1071)
classifier_nb = naiveBayes(TripType~.,
data = training_set2)
pred_nb = predict(classifier_nb,test_set2)
cm_nb = table(test_set1[,1], pred_nb)
cm_nb
#install.packages('caret')
library(caret)
confusionMatrix(cm_nb)
summary(train_data)
View(train_data)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
sum(is.na(train_data$Upc))
row.has.na <- apply(train_data, 1, function(x){train_data$Upc(is.na(x))})
sum(row.has.na)
train_data <- train_data[!row.has.na,]
rownames(train_data) <- seq(length=nrow(train_data))
summary(train_data)
sum(is.na(train_data$Upc))
row.has.na <- apply(train_data, 1, function(x){any(is.na(x))})
sum(row.has.na)
train_data <- train_data[!row.has.na,]
rownames(train_data) <- seq(length=nrow(train_data))
summary(train_data)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
#############################DATA PREPROCESSING#########################################
sum(is.na(train_data$Upc))
row.has.na <- apply(train_data$Upc, 1, function(x){any(is.na(x))})
sum(row.has.na)
train_data <- train_data[!row.has.na,]
rownames(train_data) <- seq(length=nrow(train_data))
summary(train_data)
sum(is.na(train_data$FinelineNumber))
library(caTools)
set.seed(123)
split = sample.split(train_data$TripType, SplitRatio = 0.80)
training_set2 = subset(train_data, split == TRUE)
test_set1 = subset(train_data, split == FALSE)
test_set2 <- test_set1[,2:7]
library(e1071)
classifier_nb = naiveBayes(TripType~.,
data = training_set2)
pred_nb = predict(classifier_nb,test_set2)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
sum(is.na(train_data$FinelineNumber))
row.has.na <- apply(train_data, 1, function(x){any(is.na(x))})
sum(row.has.na)
train_data <- train_data[!row.has.na,]
rownames(train_data) <- seq(length=nrow(train_data))
summary(train_data)
library(caTools)
set.seed(123)
split = sample.split(train_data$TripType, SplitRatio = 0.80)
training_set2 = subset(train_data, split == TRUE)
test_set1 = subset(train_data, split == FALSE)
test_set2 <- test_set1[,2:7]
library(e1071)
classifier_nb = naiveBayes(TripType~.,
data = training_set2)
pred_nb = predict(classifier_nb,test_set2)
cm_nb = table(test_set1[,1], pred_nb)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
library(caTools)
set.seed(123)
split = sample.split(train_data$TripType, SplitRatio = 0.80)
training_set2 = subset(train_data, split == TRUE)
test_set1 = subset(train_data, split == FALSE)
test_set2 <- test_set1[,2:7]
####################################NAIVE BAYES#########################################
library(e1071)
classifier_nb = naiveBayes(TripType~.,
data = training_set2)
pred_nb = predict(classifier_nb,test_set2)
cm_nb = table(test_set1[,1], pred_nb)
cm_nb
#install.packages('caret')
library(caret)
confusionMatrix(cm_nb)
View(test_set1)
train_data <- read.csv('train.csv')
test_data <- read.csv('test.csv')
train_data.factor <- factor(train_data$TripType)
train_data$TripType <- as.numeric(train_data.factor)
train_data$TripType <- train_data$TripType - 1
#############################DATA PREPROCESSING#########################################
sum(is.na(train_data$FinelineNumber))
row.has.na <- apply(train_data, 1, function(x){any(is.na(x))})
sum(row.has.na)
train_data <- train_data[!row.has.na,]
rownames(train_data) <- seq(length=nrow(train_data))
summary(train_data)
#############################DATA SPLITTING#############################################
library(caTools)
set.seed(123)
split = sample.split(train_data$TripType, SplitRatio = 0.80)
training_set2 = subset(train_data, split == TRUE)
test_set1 = subset(train_data, split == FALSE)
test_set2 <- test_set1[,2:7]
rownames(training_set2) <- seq(length=nrow(training_set2))
rownames(test_set1) <- seq(length=nrow(test_set1))
rownames(test_set2) <- seq(length=nrow(test_set2))
library(e1071)
classifier_nb = naiveBayes(TripType~.,
data = training_set2)
pred_nb = predict(classifier_nb,test_set2)
cm_nb = table(test_set1[,1], pred_nb)
cm_nb
#install.packages('caret')
library(caret)
confusionMatrix(cm_nb)
View(test_set1)
unique(test_set1$TripType)
length(pred_nb)
length(test_set2)
length(test_set2$VisitNumber)
length(test_set1[,1])
